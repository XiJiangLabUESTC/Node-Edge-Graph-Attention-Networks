{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "device=torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "cpac_root='/media/D/yazid/ASD-classification-ANEGCN/ABIDEI_CPAC/'\n",
    "smri_root='/media/D/yazid/ASD-classification-ANEGCN/ABIDEI_sMRI/'\n",
    "nan_subid=np.load('nan_subid.npy').tolist()\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random_seed = 7777\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention,self).__init__()\n",
    "        self.conv1=nn.Conv1d(in_channels=3,out_channels=3,kernel_size=1,padding=0)\n",
    "        self.conv2=nn.Conv1d(in_channels=116,out_channels=116,kernel_size=1,padding=0)\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "    def forward(self,Z,X):\n",
    "        K=self.conv1(X.permute(0,2,1))\n",
    "        Q=K.permute(0,2,1)\n",
    "        V=self.conv2(Z.permute(0,2,1))\n",
    "        attention=self.softmax(torch.matmul(Q,K))\n",
    "        out=torch.bmm(attention,V).permute(0,2,1)\n",
    "        return out\n",
    "class NEGAN(nn.Module):\n",
    "    def __init__(self,layer,dropout_rate):\n",
    "        super(NEGAN,self).__init__()\n",
    "        self.layer =layer\n",
    "        self.relu  =nn.ReLU()\n",
    "        self.atten =nn.ModuleList([Attention() for i in range(layer)])\n",
    "        self.norm_n=nn.ModuleList([nn.BatchNorm1d(116) for i in range(layer)])\n",
    "        self.norm_e=nn.ModuleList([nn.BatchNorm1d(116) for i in range(layer)])\n",
    "        self.node_w=nn.ParameterList([nn.Parameter(torch.randn((3,3),dtype=torch.float32)) for i in range(layer)])\n",
    "        self.edge_w=nn.ParameterList([nn.Parameter(torch.randn((116,116),dtype=torch.float32)) for i in range(layer)])\n",
    "        self.line_n=nn.ModuleList([nn.Sequential(nn.Linear(116*3,128),nn.ReLU(),nn.BatchNorm1d(128)) for i in range(layer+1)])\n",
    "        self.line_e=nn.ModuleList([nn.Sequential(nn.Linear(116*116,128*3),nn.ReLU(),nn.BatchNorm1d(128*3)) for i in range(layer+1)])\n",
    "        self.clase =nn.Sequential(nn.Linear(128*4*(self.layer+1),1024),nn.Dropout(dropout_rate),nn.ReLU(),\n",
    "                                   nn.Linear(1024,2))\n",
    "        self.ones=nn.Parameter(torch.ones((116),dtype=torch.float32),requires_grad=False)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    # params initialization\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv1d,nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def normalized(self,Z):\n",
    "        n=Z.size()[0]\n",
    "        A=Z[0,:,:]\n",
    "        A=A+torch.diag(self.ones)\n",
    "        d=A.sum(1)\n",
    "        D=torch.diag(torch.pow(d,-1))\n",
    "        A=D.mm(A).reshape(1,116,116)\n",
    "        for i in range(1,n):\n",
    "            A1=Z[i,:,:]+torch.diag(self.ones)\n",
    "            d=A1.sum(1)\n",
    "            D=torch.diag(torch.pow(d,-1))\n",
    "            A1=D.mm(A1).reshape(1,116,116)\n",
    "            A=torch.cat((A,A1),0)\n",
    "        return A\n",
    "        \n",
    "    def update_A(self,Z):\n",
    "        n=Z.size()[0]\n",
    "        A=Z[0,:,:]\n",
    "        Value,_=torch.topk(torch.abs(A.view(-1)),int(116*116*0.2))\n",
    "        A=(torch.abs(A)>=Value[-1])+torch.tensor(0,dtype=torch.float32)\n",
    "        A=A.reshape(1,116,116)\n",
    "        for i in range(1,n):\n",
    "            A2=Z[i,:,:]\n",
    "            Value,_=torch.topk(torch.abs(A2.view(-1)),int(116*116*0.2))\n",
    "            A2=(torch.abs(A2)>=Value[-1])+torch.tensor(0,dtype=torch.float32)\n",
    "            A2=A2.reshape(1,116,116)\n",
    "            A=torch.cat((A,A2),0)\n",
    "        return A\n",
    "        \n",
    "    def forward(self,X,Z):\n",
    "        n=X.size()[0]\n",
    "        XX=self.line_n[0](X.view(n,-1))\n",
    "        ZZ=self.line_e[0](Z.view(n,-1))\n",
    "        for i in range(self.layer):\n",
    "            A=self.atten[i](Z,X)\n",
    "            Z1=torch.matmul(A,Z)\n",
    "            Z2=torch.matmul(Z1,self.edge_w[i])\n",
    "            Z=self.relu(self.norm_e[i](Z2))+Z\n",
    "            ZZ=torch.cat((ZZ,self.line_e[i+1](Z.view(n,-1))),dim=1)\n",
    "            X1=torch.matmul(A,X)\n",
    "            X1=torch.matmul(X1,self.node_w[i])\n",
    "            X=self.relu(self.norm_n[i](X1))+X\n",
    "            #X.register_hook(grad_X_hook)\n",
    "            #feat_X_hook(X)\n",
    "            XX=torch.cat((XX,self.line_n[i+1](X.view(n,-1))),dim=1)\n",
    "        XZ=torch.cat((XX,ZZ),1)\n",
    "        y=self.clase(XZ)\n",
    "        #print(self.clase[0].weight)\n",
    "        return y\n",
    "def grad_X_hook(grad):\n",
    "    X_grad.append(grad)\n",
    "def feat_X_hook(X):\n",
    "    X_feat.append(X.detach())\n",
    "X_grad=list()\n",
    "X_feat=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super(LabelSmoothLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        log_prob = F.log_softmax(input, dim=-1)\n",
    "        weight = input.new_ones(input.size()) * \\\n",
    "            self.smoothing / (input.size(-1) - 1.)\n",
    "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
    "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
    "        return loss\n",
    "def data_split(full_list, ratio, shuffle=True):\n",
    "    \"\"\"\n",
    "    数据集拆分: 将列表full_list按比例ratio（随机）划分为2个子列表sublist_1与sublist_2\n",
    "    :param full_list: 数据列表\n",
    "    :param ratio:     子列表1\n",
    "    :param shuffle:   子列表2\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_total = len(full_list)\n",
    "    offset = int(n_total * ratio)\n",
    "    if n_total == 0 or offset < 1:\n",
    "        return [], full_list\n",
    "    if shuffle:\n",
    "        random.shuffle(full_list)\n",
    "    sublist_1 = full_list[:offset]\n",
    "    sublist_2 = full_list[offset:]\n",
    "    return sublist_1, sublist_2\n",
    "def data_2_k(full_list,k,shuffle=True):\n",
    "    n_total=len(full_list)\n",
    "    if shuffle:\n",
    "        random.shuffle(full_list)\n",
    "    data_list_list=[]\n",
    "    for i in range(k):\n",
    "        data_list_list.append(full_list[int(i*n_total/k):int((i+1)*n_total/k)])\n",
    "    return data_list_list\n",
    "def test(device,model,testloader):\n",
    "    model.eval()\n",
    "    TP_test,TN_test,FP_test,FN_test=0,0,0,0\n",
    "    with torch.no_grad():\n",
    "        for (X,Z,label,sub_id) in testloader:\n",
    "            TP,TN,FN,FP=0,0,0,0\n",
    "            n=X.size()[0]\n",
    "            X=X.to(device)\n",
    "            Z=Z.to(device)\n",
    "            label=label.to(device)\n",
    "            y=model(X,Z)\n",
    "            _,predict=torch.max(y,1)\n",
    "            TP+=((predict==1)&(label==1)).sum().item()\n",
    "            TN+=((predict==0)&(label==0)).sum().item()\n",
    "            FN+=((predict==0)&(label==1)).sum().item()\n",
    "            FP+=((predict==1)&(label==0)).sum().item()\n",
    "            TP_test+=TP\n",
    "            TN_test+=TN\n",
    "            FP_test+=FP\n",
    "            FN_test+=FN\n",
    "        acc,f1=cal_evaluate(TP_test,TN_test,FP_test,FN_test)\n",
    "        global max_acc\n",
    "        global modelname\n",
    "        global savedModel\n",
    "        if acc>=max_acc:\n",
    "            max_acc=acc\n",
    "            if saveModel:\n",
    "                torch.save(model.state_dict(),modelname)\n",
    "                ##read\n",
    "                #model=NERESGCN(layer)\n",
    "                #model.load_state_dict(torch.load(PATH))\n",
    "            #print('Saved the model')\n",
    "            #print('TEST:  ACC:%.4f  F1:%.4f  [TP:%3d|TN:%3d|FP:%3d|FN:%3d]'%(acc,f1,TP_test,TN_test,FP_test,FN_test)) \n",
    "        return acc,f1,TP_test,TN_test,FP_test,FN_test\n",
    "#计算边节点的字典\n",
    "def gradient(device,model,dataloader):\n",
    "    model.eval()\n",
    "    for (X,Z,A,label,sub_id) in dataloader:\n",
    "        X=torch.autograd.Variable(X,requires_grad=True)\n",
    "        x=X.to(device)\n",
    "        Z=torch.autograd.Variable(Z,requires_grad=True)\n",
    "        z=Z.to(device)\n",
    "        A=torch.autograd.Variable(A,requires_grad=True)\n",
    "        a=A.to(device)\n",
    "        y=model(x,z,a)\n",
    "        if (label==torch.FloatTensor([0])).item():\n",
    "            print('0')\n",
    "            #y.autograd.backward(torch.FloatTensor([[1.,0.]]).to(device))\n",
    "            torch.autograd.backward(y,torch.FloatTensor([[1.,0.]]).to(device))\n",
    "        else:\n",
    "            print('1')\n",
    "            torch.autograd.backward(y,torch.FloatTensor([[0.,1.]]).to(device))\n",
    "        grad_X=X.grad\n",
    "        grad_Z=Z.grad\n",
    "        #print(grad_X)\n",
    "        value_x,index_x=torch.topk(torch.abs(grad_X.view(-1)),10)\n",
    "        grad_X_topk=(torch.abs(grad_X)>=value_x[-1])\n",
    "        value_z,index_z=torch.topk(torch.abs(grad_Z.view(-1)),100)\n",
    "        grad_Z_topk=(torch.abs(grad_Z)>=value_z[-1])\n",
    "        global gradsave_dict\n",
    "        if label==torch.FloatTensor([0]).item():\n",
    "            np.save(gradsave_dict+'/TDC/Z/'+str(sub_id.item()),grad_Z.numpy())\n",
    "            np.save(gradsave_dict+'/TDC/X/'+str(sub_id.item()),grad_X.numpy())\n",
    "        else:\n",
    "            np.save(gradsave_dict+'/ASD/Z/'+str(sub_id.item()),grad_Z.numpy())\n",
    "            np.save(gradsave_dict+'/ASD/X/'+str(sub_id.item()),grad_X.numpy())\n",
    "            \n",
    "def cal_dict():\n",
    "    index=0\n",
    "    A={}\n",
    "    for i in range(116):\n",
    "        for j in range(i+1,116):\n",
    "            A[index]=(i,j)\n",
    "            A[(i,j)]=index\n",
    "            index+=1\n",
    "    return A\n",
    "def cal_evaluate(TP,TN,FP,FN):\n",
    "    if TP>0:\n",
    "        p = TP / (TP + FP)\n",
    "        r = TP / (TP + FN)\n",
    "        F1 = 2 * r * p / (r + p)\n",
    "    else:\n",
    "        F1=0\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    #print('ACC:%.4f  F1:%.4f  [TP:%d|TN:%d|FP:%d|FN:%d]'%(acc,F1,TP,TN,FP,FN))\n",
    "    return acc,F1\n",
    "def data_arange(sites,fmri_root,smri_root,nan_subid):\n",
    "    asd,tdc=[],[]\n",
    "    for site in sites:\n",
    "        mri_asd=os.listdir(smri_root+site+'/group1')\n",
    "        mri_tdc=os.listdir(smri_root+site+'/group2')\n",
    "        fmri_asd=os.listdir(fmri_root+site+'/group1_FC')\n",
    "        fmri_tdc=os.listdir(fmri_root+site+'/group2_FC')\n",
    "        site_asd=[i for i in mri_asd if i in fmri_asd ]\n",
    "        site_tdc=[i for i in mri_tdc if i in fmri_tdc ]\n",
    "        site_asd=[i for i in site_asd if int(i[:5]) not in nan_subid]\n",
    "        site_tdc=[i for i in site_tdc if int(i[:5]) not in nan_subid]\n",
    "        asd.append(site_asd)\n",
    "        tdc.append(site_tdc)\n",
    "    return asd,tdc\n",
    "class dataset(Dataset):\n",
    "    def __init__(self,fmri_root,smri_root,site,ASD,TDC,edge_dict=cal_dict(),topk=True,rate=0.2):\n",
    "        super(dataset,self).__init__()\n",
    "        self.fmri=fmri_root\n",
    "        self.smri=smri_root\n",
    "        self.ASD=[j for i in ASD for j in i]\n",
    "        self.TDC=[j for i in TDC for j in i]\n",
    "        self.data=self.ASD+self.TDC\n",
    "        random.shuffle(self.data)\n",
    "        self.data_site={}\n",
    "        for i in range(len(site)):\n",
    "            data=ASD[i]+TDC[i]\n",
    "            for j in data:\n",
    "                if j not in self.data_site:\n",
    "                    self.data_site[j]=site[i]                \n",
    "        self.dict=edge_dict\n",
    "        self.rate=rate\n",
    "        self.topk=topk\n",
    "    def normalize(self,A):\n",
    "        d=A.sum(1)\n",
    "        D=torch.diag(torch.pow(d,-1))\n",
    "        return D.mm(A)\n",
    "    def __getitem__(self,index):\n",
    "        data=self.data[index]\n",
    "        sub_id=int(data[0:5])\n",
    "        if data in self.ASD:\n",
    "            data_slow5 =np.load(self.fmri+self.data_site[data]+'/group1_slow5/'+data,allow_pickle=True)\n",
    "            data_slow4 =np.load(self.fmri+self.data_site[data]+'/group1_slow4/'+data,allow_pickle=True)\n",
    "            data_voxel =np.load(self.smri+self.data_site[data]+'/group1/'+data,allow_pickle=True)\n",
    "            data_FCz   =np.load(self.fmri+self.data_site[data]+'/group1_FC/'+data,allow_pickle=True)\n",
    "        elif data in self.TDC:\n",
    "            data_slow5 =np.load(self.fmri+self.data_site[data]+'/group2_slow5/'+data,allow_pickle=True)\n",
    "            data_slow4 =np.load(self.fmri+self.data_site[data]+'/group2_slow4/'+data,allow_pickle=True)\n",
    "            data_voxel =np.load(self.smri+self.data_site[data]+'/group2/'+data,allow_pickle=True)\n",
    "            data_FCz   =np.load(self.fmri+self.data_site[data]+'/group2_FC/'+data,allow_pickle=True)\n",
    "        else:\n",
    "            print('wrong input')\n",
    "        data_slow5=(data_slow5-np.min(data_slow5))/(np.max(data_slow5)-np.min(data_slow5))\n",
    "        data_slow4=(data_slow4-np.min(data_slow4))/(np.max(data_slow4)-np.min(data_slow4))\n",
    "        if np.any(np.isnan(data_slow5)) or np.any(np.isnan(data_slow4)) or np.any(np.isnan(data_FCz)):\n",
    "            print('data wronmg')\n",
    "        #data_FCz=(data_FCz-np.min(data_FCz))/(np.max(data_FCz)-np.min(data_FCz))\n",
    "        if self.data[index] in self.ASD:\n",
    "            label=torch.tensor(1)\n",
    "        else:\n",
    "            label=torch.tensor(0)\n",
    "        X=np.zeros((116,3),dtype=np.float32)\n",
    "        X[:,0]=data_slow5\n",
    "        X[:,1]=data_slow4\n",
    "        X[:,2]=data_voxel\n",
    "        data_FCz=data_FCz.astype(np.float32)\n",
    "        Z=torch.from_numpy(data_FCz)\n",
    "        X=torch.from_numpy(X)\n",
    "        return X,Z,label,sub_id\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "def get_acc(acc_list,toprate):\n",
    "    acc_list.sort()\n",
    "    return acc_list[-int(toprate*len(acc_list))]\n",
    "def plot_acc(acc_list):\n",
    "    num_bins=50\n",
    "    fig,ax=plt.subplots(2)\n",
    "    n,bins,patches=ax[0].hist(acc_list,num_bins,density=True)\n",
    "    ax[1].plot(acc_list)\n",
    "    ax[1].set_ylim(0.4,1)\n",
    "    plt.show()\n",
    "    print('Top:10%:',get_acc(acc_list,0.1))\n",
    "    print('Top:20%:',get_acc(acc_list,0.2))\n",
    "    print('Max:    ',max(acc_list))\n",
    "def train_fgsm(model,trainloader,testloader,epsilon):\n",
    "    result=pd.DataFrame(columns=('Loss','Acc','F1','TP','TN','FP','FN'))\n",
    "    criterian1=LabelSmoothLoss(0.1).to(device)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=gmma)\n",
    "    acc=0.5000\n",
    "    loss_sum=0\n",
    "    for j in range(epoch):\n",
    "        print('\\r[%3d/%3d] Loss: %.2f  Acc:%.4f' %(j+1,epoch,loss_sum,acc),end='')\n",
    "        loss_sum=0\n",
    "        TP,TN,FP,FN=0,0,0,0\n",
    "        for (X,Z,A,label,sub_id) in trainloader:\n",
    "            model.train()\n",
    "            x=X.to(device)\n",
    "            z=Z.to(device)\n",
    "            x.requires_grad=True\n",
    "            z.requires_grad=True\n",
    "            label=label.to(device)\n",
    "            y=model(x,z)\n",
    "            loss=criterian1(y,label)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            sign_grad_x=torch.sign(x.grad.data)\n",
    "            sign_grad_z=torch.sign(z.grad.data)\n",
    "            perturbed_x=x+epsilon*sign_grad_x \n",
    "            perturbed_z=z+epsilon*sign_grad_z \n",
    "            perturbed_x=torch.clamp(perturbed_x,0,1)\n",
    "            perturbed_z=torch.clamp(perturbed_z,-1,1)\n",
    "            y=model(perturbed_x,perturbed_z)\n",
    "            L2=torch.tensor(0,dtype=torch.float32).to(device)\n",
    "            if L2_lamda>0:\n",
    "                for name,parameters in model.named_parameters():\n",
    "                    if name[0:5]=='clase' and  name[-8:]=='0.weight':\n",
    "                        L2+=L2_lamda*torch.norm(parameters,2)\n",
    "            loss=0.5*(criterian1(y,label)+loss)+L2\n",
    "            loss_sum+=loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc,f1,TP_test,TN_test,FP_test,FN_test=test(device,model,testloader)\n",
    "        result.loc[j]={'Loss':loss_sum,'Acc':acc,'F1':f1,'TP':TP_test,'TN':TN_test,'FP':FP_test,'FN':FN_test}\n",
    "    result.sort_values('Acc',inplace=True,ascending=False)\n",
    "    print('\\n')\n",
    "    return result.iloc[9]\n",
    "def train(model,trainloader,testloader):\n",
    "    result=pd.DataFrame(columns=('Loss','Acc','F1','TP','TN','FP','FN'))\n",
    "    criterian1=LabelSmoothLoss(0.1).to(device)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    #optimizer = AdaBelief(model.parameters(), lr=1e-4, eps=1e-8, betas=(0.9,0.999), weight_decay=L2_lamda,weight_decouple = True, rectify = False)\n",
    "    scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=gmma)\n",
    "    acc=0.5000\n",
    "    loss_sum=0\n",
    "    for _ in range(epoch):\n",
    "        print('\\rLoss: {:.2f}  Acc:{:.4f}'.format(loss_sum,acc),end='')\n",
    "        loss_sum=0\n",
    "        TP,TN,FP,FN=0,0,0,0\n",
    "        time_start=time.time()\n",
    "        for (X,Z,A,label,sub_id) in trainloader:\n",
    "            #print(A)\n",
    "            #print(Z)\n",
    "            #print(X.shape,torch.mean(X),torch.std(X))\n",
    "            #X=X+torch.randn(X.shape)*X.std(0)\n",
    "            #Z=Z+torch.randn(Z.shape)*Z.std(0)\n",
    "            model.train()\n",
    "            X=X.to(device)\n",
    "            Z=Z.to(device)\n",
    "            label=label.to(device)\n",
    "            y=model(X,Z)\n",
    "            #print(y)\n",
    "            _,predict=torch.max(y,1)\n",
    "            TP+=((predict==1)&(label==1)).sum().item()\n",
    "            TN+=((predict==0)&(label==0)).sum().item()\n",
    "            FN+=((predict==0)&(label==1)).sum().item()\n",
    "            FP+=((predict==1)&(label==0)).sum().item()\n",
    "            loss=criterian1(y,label)\n",
    "            L1=torch.tensor(0,dtype=torch.float32).to(device)\n",
    "            L2=torch.tensor(0,dtype=torch.float32).to(device)\n",
    "            #print(model.parameters.weit_n)\n",
    "            if L1_lamda>0 or L2_lamda>0:\n",
    "                for name,parameters in model.named_parameters():\n",
    "                    if name[0:5]=='clase' and  name[-8:]=='0.weight':\n",
    "                        L1+=L1_lamda*torch.norm(parameters,1)\n",
    "                        L2+=L2_lamda*torch.norm(parameters,2)\n",
    "            loss+=(L1+L2)\n",
    "            loss_sum+=loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        time_end=time.time()\n",
    "        time_cost=(time_end-time_start)/60.0\n",
    "        acc,f1,TP_test,TN_test,FP_test,FN_test=test(device,model,testloader)\n",
    "        result.loc[j]={'Loss':loss_sum,'Acc':acc,'F1':f1,'TP':TP_test,'TN':TN_test,'FP':FP_test,'FN':FN_test}\n",
    "        #acc,f1=cal_evaluate(TP,TN,FP,FN)\n",
    "        #print(\"[%2d/%d] ACC:%.2f F1:%.2f Loss: %.4f [TP:%3d|TN:%3d|FP:%3d|FN:%3d] CostTime:%4.1f min | RestTime:%.2f h\" %(\n",
    "         #   j+1,epoch,acc,f1,loss_sum,TP,TN,FP,FN,time_cost,time_cost/60*(epoch-1-j)))\n",
    "    #losses.append(loss_sum)\n",
    "    #print(model.parameters())\n",
    "        #acc,f1=test(device,model,testloader)\n",
    "        #print(acc)\n",
    "    #plot_acc(result['Acc'],result['Loss'])\n",
    "    result.sort_values('Acc',inplace=True,ascending=False)\n",
    "    print('\\n')\n",
    "    return result.iloc[9] \n",
    "def train_pgd(model,trainloader,testloader,eps=0.05,iters=10,alpha=2/255):\n",
    "    result=pd.DataFrame(columns=('Loss','Acc','F1','TP','TN','FP','FN'))\n",
    "    criterian1=LabelSmoothLoss(0.1).to(device)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    for j in range(epoch):\n",
    "        loss_sum=0.\n",
    "        TP,TN,FP,FN=0,0,0,0\n",
    "        model.train()\n",
    "        for (X,Z,label,sub_id) in trainloader:\n",
    "            model.train()\n",
    "            x=X.to(device)\n",
    "            z=Z.to(device)\n",
    "            label=label.to(device)\n",
    "            pretu_x,pretu_z=x,z\n",
    "            ori_x,ori_z=x.data,z.data\n",
    "            for i in range(iters):\n",
    "                pretu_x.requires_grad=True\n",
    "                pretu_z.requires_grad=True\n",
    "                y=model(pretu_x,pretu_z)\n",
    "                loss=criterian1(y,label)\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                adv_x=pretu_x+alpha*torch.sign(pretu_x.grad.data)\n",
    "                adv_z=pretu_z+alpha*torch.sign(pretu_z.grad.data)\n",
    "                eta_x=torch.clamp(adv_x-ori_x,min=-eps,max=eps)\n",
    "                eta_z=torch.clamp(adv_z-ori_z,min=-eps,max=eps)\n",
    "                pretu_x=torch.clamp(ori_x+eta_x,min=0,max=1).detach_()\n",
    "                pretu_z=torch.clamp(ori_z+eta_z,min=-1,max=1).detach_()\n",
    "            y=model(x,z)\n",
    "            yy=model(pretu_x,pretu_z)\n",
    "            L2=torch.tensor(0,dtype=torch.float32).to(device)\n",
    "            if L2_lamda>0:\n",
    "                for name,parameters in model.named_parameters():\n",
    "                    if name[0:5]=='clase' and  name[-8:]=='0.weight':\n",
    "                        L2+=L2_lamda*torch.norm(parameters,2)\n",
    "            loss=0.5*(criterian1(yy,label)+criterian1(y,label))+L2\n",
    "            loss_sum+=loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (j+1)%10==0:\n",
    "            acc,f1,TP_test,TN_test,FP_test,FN_test=test(device,model,testloader)\n",
    "            result.loc[(j+1)//10]=[loss_sum,acc,f1,TP_test,TN_test,FP_test,FN_test]\n",
    "    result.sort_values('Acc',inplace=True,ascending=False)\n",
    "    print(' FinalAcc: {:.4f}'.format(result.iloc[0]['Acc']))\n",
    "    return result.iloc[0]['Acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FinalAcc: 0.6875\n",
      " FinalAcc: 0.6774\n",
      " FinalAcc: 0.7017\n",
      " FinalAcc: 0.7778\n",
      " FinalAcc: 0.8462\n",
      " FinalAcc: 0.7900\n",
      " FinalAcc: 0.6667\n",
      " FinalAcc: 0.6182\n",
      " FinalAcc: 0.6078\n",
      " FinalAcc: 0.6389\n",
      " FinalAcc: 0.7708\n",
      " FinalAcc: 0.7000\n",
      " FinalAcc: 0.6909\n",
      " FinalAcc: 0.7500\n",
      " FinalAcc: 0.7172\n",
      " FinalAcc: 0.7895\n",
      " FinalAcc: 0.6000\n",
      "dropout_rate:0.0010   Acc:0.7001\n",
      " FinalAcc: 0.6806\n",
      " FinalAcc: 0.6452\n",
      " FinalAcc: 0.7127\n",
      " FinalAcc: 0.8889\n",
      " FinalAcc: 0.8462\n",
      " FinalAcc: 0.7700\n",
      " FinalAcc: 0.6296\n",
      " FinalAcc: 0.6727\n",
      " FinalAcc: 0.6667\n",
      " FinalAcc: 0.6944\n",
      " FinalAcc: 0.6667\n",
      " FinalAcc: 0.7500\n",
      " FinalAcc: 0.7455\n",
      " FinalAcc: 0.6786\n",
      " FinalAcc: 0.7172\n",
      " FinalAcc: 0.8421\n",
      " FinalAcc: 0.6250\n",
      "dropout_rate:0.0050   Acc:0.7051\n",
      " FinalAcc: 0.7292\n",
      " FinalAcc: 0.6774\n",
      " FinalAcc: 0.7017\n",
      " FinalAcc: 0.7778\n",
      " FinalAcc: 0.8462\n",
      " FinalAcc: 0.8000\n",
      " FinalAcc: 0.8148\n",
      " FinalAcc: 0.6364\n",
      " FinalAcc: 0.7059\n",
      " FinalAcc: 0.7222\n",
      " FinalAcc: 0.7292\n",
      " FinalAcc: 0.8000\n",
      " FinalAcc: 0.7818\n",
      " FinalAcc: 0.6786\n",
      " FinalAcc: 0.7374\n",
      " FinalAcc: 0.8421\n",
      " FinalAcc: 0.6000\n",
      "dropout_rate:0.0100   Acc:0.7279\n",
      " FinalAcc: 0.7014\n",
      " FinalAcc: 0.6613\n",
      " FinalAcc: 0.7459\n",
      " FinalAcc: 0.6667\n",
      " FinalAcc: 0.7692\n",
      " FinalAcc: 0.7800\n",
      " FinalAcc: 0.7407\n",
      " FinalAcc: 0.6727\n",
      " FinalAcc: 0.5882\n",
      " FinalAcc: 0.7222\n",
      " FinalAcc: 0.6667\n",
      " FinalAcc: 0.7500\n",
      " FinalAcc: 0.7455\n",
      " FinalAcc: 0.6071\n",
      " FinalAcc: 0.7071\n",
      " FinalAcc: 0.8421\n",
      " FinalAcc: 0.6500\n",
      "dropout_rate:0.0200   Acc:0.7110\n"
     ]
    }
   ],
   "source": [
    "# leave one site out\n",
    "setup_seed(random_seed)\n",
    "L1_lamda=0.0\n",
    "L2_lamda=0.0001\n",
    "learning_rate=0.0001\n",
    "epoch   =100\n",
    "batch_size=64\n",
    "gmma    =0.95\n",
    "layer   =5\n",
    "all_site=os.listdir(cpac_root)\n",
    "site_count={'CALTECH':19,'CMU':13,'KKI':51,'LEUVEN':62,'MAXMUN':55,'NYU':181,\n",
    "           'OHSU':28,'OLIN':36,'PITT':40,'SBL':9,'SDSU':27,'STANFORD':40,\n",
    "           'TRINITY':48,'UCLA':99,'UM':144,'USM':100,'YALE':55}\n",
    "for epision in [0.001,0.005,0.01,0.02]:\n",
    "    sum_acc=0\n",
    "    for site in all_site:\n",
    "        test_site =[i for i in all_site if i==site]\n",
    "        train_site=[i for i in all_site if i!=site]\n",
    "        global max_acc\n",
    "        max_acc=0.5\n",
    "        global modelname\n",
    "        modelname='/media/dm/0001A094000BF891/Yazid/SAVEDModels/adversialtrained/models_{}'.format(test_site[0])\n",
    "        global saveModel\n",
    "        saveModel=False\n",
    "        train_asd,train_tdc=data_arange(train_site,fmri_root=cpac_root,smri_root=smri_root,nan_subid=nan_subid)\n",
    "        test_asd,test_tdc  =data_arange(test_site, fmri_root=cpac_root,smri_root=smri_root,nan_subid=nan_subid)\n",
    "        trainset=dataset(site=train_site,fmri_root=cpac_root,smri_root=smri_root,ASD=train_asd,TDC=train_tdc,edge_dict=cal_dict())\n",
    "        trainloader=DataLoader(trainset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "        testset=dataset(site=test_site,fmri_root=cpac_root,smri_root=smri_root,ASD=test_asd,TDC=test_tdc,edge_dict=cal_dict())\n",
    "        testloader=DataLoader(testset,batch_size=1)\n",
    "        model=NEResGCN(layer,dropout_rate=0.2).to(device)\n",
    "        acc=train_pgd(model,trainloader,testloader,eps=epision,iters=10,alpha=epision/5)\n",
    "        sum_acc+=(site_count[test_site[0]]*acc)\n",
    "    print('dropout_rate:%.4f   Acc:%.4f'%(epision,sum_acc/1007))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "''' \n",
    "to find nan_subject_id \n",
    "'''\n",
    "nan_subid=[]\n",
    "for index in range(10):\n",
    "    test_asd =test_asd_dict[index]\n",
    "    test_tdc =test_tdc_dict[index]\n",
    "    testset=dataset(site=test_site,fmri_root=cpac_root,smri_root=smri_root,ASD=test_asd,TDC=test_tdc,edge_dict=cal_dict())\n",
    "    testloader=DataLoader(testset,batch_size=1)\n",
    "    model=NEResGCN(5)\n",
    "    model.eval()\n",
    "    for (X,Z,A,label,sub_id) in testloader:\n",
    "        sub_id=sub_id.item()\n",
    "        y=model(X,Z,A)\n",
    "        if torch.any(torch.isnan(y)):\n",
    "            print(sub_id)\n",
    "            nan_subid.append(sub_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon**:***0.0010*** **Acc**: ***703***<br>\n",
    "**Epsilon**:***0.0050*** **Acc**: ***715***<br>\n",
    "**Epsilon**:***0.0100*** **Acc**: ***715***<br>\n",
    "**Epsilon**:***0.0200*** **Acc**: ***727***<br>\n",
    "**Epsilon**:***0.0500*** **Acc**: ***717***<br>\n",
    "**Epsilon**:***0.1000*** **Acc**: ***675***<br>\n",
    "**Epsilon**:***0.2000*** **Acc**: ***627***<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
