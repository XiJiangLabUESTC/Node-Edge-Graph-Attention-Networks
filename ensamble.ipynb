{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "device=torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "from tqdm import tqdm\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cpac_root='/media/D/yazid/ASD-classification-ANEGCN/ABIDEI_CPAC/'\n",
    "smri_root='/media/D/yazid/ASD-classification-ANEGCN/ABIDEI_sMRI/'\n",
    "nan_subid=np.load('nan_subid.npy').tolist()\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention,self).__init__()\n",
    "        self.conv1=nn.Conv1d(in_channels=3,out_channels=3,kernel_size=1,padding=0)\n",
    "        self.conv2=nn.Conv1d(in_channels=116,out_channels=116,kernel_size=1,padding=0)\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "    def forward(self,Z,X):\n",
    "        batchsize,x_dim,x_c= X.size()\n",
    "        batchsize,z_dim,z_c= Z.size()\n",
    "        K=self.conv1(X.permute(0,2,1))# BS,x_c,x_dim\n",
    "        Q=K.permute(0,2,1)# BS,x_dim,x_c\n",
    "        V=self.conv2(Z.permute(0,2,1))# Bs,z_c,z_dim\n",
    "        attention=self.softmax(torch.matmul(Q,K))#BS,x_dim,x_dim\n",
    "        out=torch.bmm(attention,V).permute(0,2,1)#BS,z_dim,z_c\n",
    "        return out\n",
    "class ANEGCN(nn.Module):\n",
    "    def __init__(self,layer):\n",
    "        super(ANEGCN,self).__init__()\n",
    "        self.layer =layer\n",
    "        self.relu  =nn.ReLU()\n",
    "        self.atten =nn.ModuleList([Attention() for i in range(layer)])\n",
    "        self.norm_n=nn.ModuleList([nn.BatchNorm1d(116) for i in range(layer)])\n",
    "        self.norm_e=nn.ModuleList([nn.BatchNorm1d(116) for i in range(layer)])\n",
    "        self.node_w=nn.ParameterList([nn.Parameter(torch.randn((3,3),dtype=torch.float32)) for i in range(layer)])\n",
    "        self.edge_w=nn.ParameterList([nn.Parameter(torch.randn((116,116),dtype=torch.float32)) for i in range(layer)])\n",
    "        self.line_n=nn.ModuleList([nn.Sequential(nn.Linear(116*3,128),nn.ReLU(),nn.BatchNorm1d(128)) for i in range(layer+1)])\n",
    "        self.line_e=nn.ModuleList([nn.Sequential(nn.Linear(116*116,128*3),nn.ReLU(),nn.BatchNorm1d(128*3)) for i in range(layer+1)])\n",
    "        self.clase =nn.Sequential(nn.Linear(128*4*(self.layer+1),1024),nn.Dropout(0.2),nn.ReLU(),\n",
    "                                   nn.Linear(1024,2))\n",
    "        self.ones=nn.Parameter(torch.ones((116),dtype=torch.float32),requires_grad=False)\n",
    "        self._initialize_weights()\n",
    "    # params initialization\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv1d,nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def normalized(self,Z):\n",
    "        n=Z.size()[0]\n",
    "        A=Z[0,:,:]\n",
    "        A=A+torch.diag(self.ones)\n",
    "        d=A.sum(1)\n",
    "        D=torch.diag(torch.pow(d,-1))\n",
    "        A=D.mm(A).reshape(1,116,116)\n",
    "        for i in range(1,n):\n",
    "            A1=Z[i,:,:]+torch.diag(self.ones)\n",
    "            d=A1.sum(1)\n",
    "            D=torch.diag(torch.pow(d,-1))\n",
    "            A1=D.mm(A1).reshape(1,116,116)\n",
    "            A=torch.cat((A,A1),0)\n",
    "        return A\n",
    "        \n",
    "    def update_A(self,Z):\n",
    "        n=Z.size()[0]\n",
    "        A=Z[0,:,:]\n",
    "        Value,_=torch.topk(torch.abs(A.view(-1)),int(116*116*0.2))\n",
    "        A=(torch.abs(A)>=Value[-1])+torch.tensor(0,dtype=torch.float32)\n",
    "        A=A.reshape(1,116,116)\n",
    "        for i in range(1,n):\n",
    "            A2=Z[i,:,:]\n",
    "            Value,_=torch.topk(torch.abs(A2.view(-1)),int(116*116*0.2))\n",
    "            A2=(torch.abs(A2)>=Value[-1])+torch.tensor(0,dtype=torch.float32)\n",
    "            A2=A2.reshape(1,116,116)\n",
    "            A=torch.cat((A,A2),0)\n",
    "        return A\n",
    "        \n",
    "    def forward(self,X,Z):\n",
    "        n=X.size()[0]\n",
    "        XX=self.line_n[0](X.view(n,-1))\n",
    "        ZZ=self.line_e[0](Z.view(n,-1))\n",
    "        for i in range(self.layer):\n",
    "            A=self.atten[i](Z,X)\n",
    "            Z1=torch.matmul(A,Z)\n",
    "            Z2=torch.matmul(Z1,self.edge_w[i])\n",
    "            Z=self.relu(self.norm_e[i](Z2))+Z\n",
    "            ZZ=torch.cat((ZZ,self.line_e[i+1](Z.view(n,-1))),dim=1)\n",
    "            X1=torch.matmul(A,X)\n",
    "            X1=torch.matmul(X1,self.node_w[i])\n",
    "            X=self.relu(self.norm_n[i](X1))+X\n",
    "            #X.register_hook(grad_X_hook)\n",
    "            #feat_X_hook(X)\n",
    "            XX=torch.cat((XX,self.line_n[i+1](X.view(n,-1))),dim=1)\n",
    "        XZ=torch.cat((XX,ZZ),1)\n",
    "        y=self.clase(XZ)\n",
    "        #print(self.clase[0].weight)\n",
    "        return y\n",
    "def grad_X_hook(grad):\n",
    "    X_grad.append(grad)\n",
    "def feat_X_hook(X):\n",
    "    X_feat.append(X.detach())\n",
    "X_grad=list()\n",
    "X_feat=list()\n",
    "class LabelSmoothLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super(LabelSmoothLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        log_prob = F.log_softmax(input, dim=-1)\n",
    "        weight = input.new_ones(input.size()) * \\\n",
    "            self.smoothing / (input.size(-1) - 1.)\n",
    "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
    "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_evaluate(TP,TN,FP,FN):\n",
    "    if TP>0:\n",
    "        p = TP / (TP + FP)\n",
    "        r = TP / (TP + FN)\n",
    "        F1 = 2 * r * p / (r + p)\n",
    "    else:\n",
    "        F1=0\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    #print('ACC:%.4f  F1:%.4f  [TP:%d|TN:%d|FP:%d|FN:%d]'%(acc,F1,TP,TN,FP,FN))\n",
    "    return acc,F1\n",
    "def test(device,model,testloader):\n",
    "    model.eval()\n",
    "    TP_test,TN_test,FP_test,FN_test=0,0,0,0\n",
    "    with torch.no_grad():\n",
    "        for (X,Z,label,sub_id) in testloader:\n",
    "            TP,TN,FN,FP=0,0,0,0\n",
    "            n=X.size()[0]\n",
    "            X=X.to(device)\n",
    "            Z=Z.to(device)\n",
    "            label=label.to(device)\n",
    "            y=model(X,Z)\n",
    "            _,predict=torch.max(y,1)\n",
    "            TP+=((predict==1)&(label==1)).sum().item()\n",
    "            TN+=((predict==0)&(label==0)).sum().item()\n",
    "            FN+=((predict==0)&(label==1)).sum().item()\n",
    "            FP+=((predict==1)&(label==0)).sum().item()\n",
    "            TP_test+=TP\n",
    "            TN_test+=TN\n",
    "            FP_test+=FP\n",
    "            FN_test+=FN\n",
    "        acc,f1=cal_evaluate(TP_test,TN_test,FP_test,FN_test)\n",
    "        global max_acc\n",
    "        global modelname\n",
    "        global savedModel\n",
    "        if acc>=max_acc:\n",
    "            max_acc=acc\n",
    "            if saveModel:\n",
    "                torch.save(model.state_dict(),modelname)\n",
    "        return acc,f1,TP_test,TN_test,FP_test,FN_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,fmri_root,smri_root,site,ASD,TDC):\n",
    "        super(dataset,self).__init__()\n",
    "        self.fmri=fmri_root\n",
    "        self.smri=smri_root\n",
    "        self.ASD=[j for i in ASD for j in i]\n",
    "        self.TDC=[j for i in TDC for j in i]\n",
    "        self.data=self.ASD+self.TDC\n",
    "        random.shuffle(self.data)\n",
    "        self.data_site={}\n",
    "        for i in range(len(site)):\n",
    "            data=ASD[i]+TDC[i]\n",
    "            for j in data:\n",
    "                if j not in self.data_site:\n",
    "                    self.data_site[j]=site[i]                \n",
    "    def __getitem__(self,index):\n",
    "        data=self.data[index]\n",
    "        sub_id=int(data[0:5])\n",
    "        if data in self.ASD:\n",
    "            data_slow5 =np.load(self.fmri+self.data_site[data]+'/group1_slow5/'+data,allow_pickle=True)\n",
    "            data_slow4 =np.load(self.fmri+self.data_site[data]+'/group1_slow4/'+data,allow_pickle=True)\n",
    "            data_voxel =np.load(self.smri+self.data_site[data]+'/group1/'+data,allow_pickle=True)\n",
    "            data_FCz   =np.load(self.fmri+self.data_site[data]+'/group1_FC/'+data,allow_pickle=True)\n",
    "        elif data in self.TDC:\n",
    "            data_slow5 =np.load(self.fmri+self.data_site[data]+'/group2_slow5/'+data,allow_pickle=True)\n",
    "            data_slow4 =np.load(self.fmri+self.data_site[data]+'/group2_slow4/'+data,allow_pickle=True)\n",
    "            data_voxel =np.load(self.smri+self.data_site[data]+'/group2/'+data,allow_pickle=True)\n",
    "            data_FCz   =np.load(self.fmri+self.data_site[data]+'/group2_FC/'+data,allow_pickle=True)\n",
    "        else:\n",
    "            print('wrong input')\n",
    "        data_slow5=(data_slow5-np.min(data_slow5))/(np.max(data_slow5)-np.min(data_slow5))\n",
    "        data_slow4=(data_slow4-np.min(data_slow4))/(np.max(data_slow4)-np.min(data_slow4))\n",
    "        if np.any(np.isnan(data_slow5)) or np.any(np.isnan(data_slow4)) or np.any(np.isnan(data_FCz)):\n",
    "            print('data wronmg')\n",
    "        #data_FCz=(data_FCz-np.min(data_FCz))/(np.max(data_FCz)-np.min(data_FCz))\n",
    "        if self.data[index] in self.ASD:\n",
    "            label=torch.tensor(1)\n",
    "        else:\n",
    "            label=torch.tensor(0)\n",
    "        X=np.zeros((116,3),dtype=np.float32)\n",
    "        X[:,0]=data_slow5\n",
    "        X[:,1]=data_slow4\n",
    "        X[:,2]=data_voxel\n",
    "        data_FCz=data_FCz.astype(np.float32)\n",
    "        Z=torch.from_numpy(data_FCz)\n",
    "        X=torch.from_numpy(X)\n",
    "        return X,Z,label,sub_id\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fgsm(model,trainloader,testloader,epsilon=0.05):\n",
    "    result=pd.DataFrame(columns=('Loss','Acc','F1','TP','TN','FP','FN'))\n",
    "    criterian1=LabelSmoothLoss(0.1).to(device)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=gmma)\n",
    "    acc=0.5000\n",
    "    loss_sum=0\n",
    "    for j in range(epoch):\n",
    "        print('\\rLoss: {:.2f}  Acc:{:.4f}'.format(loss_sum,acc),end='')\n",
    "        loss_sum=0\n",
    "        TP,TN,FP,FN=0,0,0,0\n",
    "        model.train()\n",
    "        for (X,Z,label,sub_id) in trainloader:\n",
    "            x=X.to(device)\n",
    "            z=Z.to(device)\n",
    "            x.requires_grad=True\n",
    "            z.requires_grad=True\n",
    "            label=label.to(device)\n",
    "            y=model(x,z)\n",
    "            loss=criterian1(y,label)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            sign_grad_x=torch.sign(x.grad.data)\n",
    "            sign_grad_z=torch.sign(z.grad.data)\n",
    "            perturbed_x=x+epsilon*sign_grad_x \n",
    "            perturbed_z=z+epsilon*sign_grad_z \n",
    "            perturbed_x=torch.clamp(perturbed_x,0,1)\n",
    "            perturbed_z=torch.clamp(perturbed_z,-1,1)\n",
    "            y=model(perturbed_x,perturbed_z)\n",
    "            L2=torch.tensor(0,dtype=torch.float32).to(device)\n",
    "            if L2_lamda>0:\n",
    "                for name,parameters in model.named_parameters():\n",
    "                    if name[0:5]=='clase' and  name[-8:]=='0.weight':\n",
    "                        L2+=L2_lamda*torch.norm(parameters,2)\n",
    "            loss=0.5*(criterian1(y,label)+loss)+L2\n",
    "            loss_sum+=loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (j+1)%10 == 0 or j==0:\n",
    "            acc,f1,TP_test,TN_test,FP_test,FN_test=test(device,model,testloader)\n",
    "        result.loc[j]={'Loss':loss_sum,'Acc':acc,'F1':f1,'TP':TP_test,'TN':TN_test,'FP':FP_test,'FN':FN_test}\n",
    "    result.sort_values('Acc',inplace=True,ascending=False)\n",
    "    print('',end='')\n",
    "    print('Acc:  ', result.iloc[0]['Acc'])\n",
    "def train_norm(model,trainloader,testloader):\n",
    "    result=pd.DataFrame(columns=('Loss','Acc','F1','TP','TN','FP','FN'))\n",
    "    criterian1=LabelSmoothLoss(0.1).to(device)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    acc=0.5000\n",
    "    loss_sum=0\n",
    "    for j in range(epoch):\n",
    "        print('\\rEPOCH: [{:03d}|100]  Loss: {:.2f}  Acc:{:.4f}'.format(j+1,loss_sum,acc),end='')\n",
    "        loss_sum=0\n",
    "        TP,TN,FP,FN=0,0,0,0\n",
    "        model.train()\n",
    "        for (X,Z,label,sub_id) in trainloader:\n",
    "            x=X.to(device)\n",
    "            z=Z.to(device)\n",
    "            label=label.to(device)\n",
    "            y=model(x,z)\n",
    "            loss=criterian1(y,label)\n",
    "            L2=torch.tensor(0,dtype=torch.float32).to(device)\n",
    "            if L2_lamda>0:\n",
    "                for name,parameters in model.named_parameters():\n",
    "                    if name[0:5]=='clase' and  name[-8:]=='0.weight':\n",
    "                        L2+=L2_lamda*torch.norm(parameters,2)\n",
    "            loss=loss+L2\n",
    "            loss_sum+=loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (j+1)%10 == 0 or j==0:\n",
    "            acc,f1,TP_test,TN_test,FP_test,FN_test=test(device,model,testloader)\n",
    "            result.loc[j//10]={'Loss':loss_sum,'Acc':acc,'F1':f1,'TP':TP_test,'TN':TN_test,'FP':FP_test,'FN':FN_test}\n",
    "    result.sort_values('Acc',inplace=True,ascending=False)\n",
    "    print(' FinalAcc: {:.4f}'.format(result.iloc[0]['Acc']))\n",
    "def train_pgd(model,trainloader,testloader,eps=0.2,iters=10,alpha=8/255):\n",
    "    result=pd.DataFrame(columns=('Loss','Acc','F1','TP','TN','FP','FN'))\n",
    "    criterian1=LabelSmoothLoss(0.1).to(device)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    for j in range(epoch):\n",
    "        loss_sum=0.\n",
    "        TP,TN,FP,FN=0,0,0,0\n",
    "        model.train()\n",
    "        for (X,Z,label,sub_id) in trainloader:\n",
    "            model.train()\n",
    "            x=X.to(device)\n",
    "            z=Z.to(device)\n",
    "            label=label.to(device)\n",
    "            pretu_x,pretu_z=x,z\n",
    "            ori_x,ori_z=x.data,z.data\n",
    "            for i in range(iters):\n",
    "                pretu_x.requires_grad=True\n",
    "                pretu_z.requires_grad=True\n",
    "                y=model(pretu_x,pretu_z)\n",
    "                loss=criterian1(y,label)\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                adv_x=pretu_x+alpha*torch.sign(pretu_x.grad.data)\n",
    "                adv_z=pretu_z+alpha*torch.sign(pretu_z.grad.data)\n",
    "                eta_x=torch.clamp(adv_x-ori_x,min=-eps,max=eps)\n",
    "                eta_z=torch.clamp(adv_z-ori_z,min=-eps,max=eps)\n",
    "                pretu_x=torch.clamp(ori_x+eta_x,min=0,max=1).detach_()\n",
    "                pretu_z=torch.clamp(ori_z+eta_z,min=-1,max=1).detach_()\n",
    "            y=model(x,z)\n",
    "            yy=model(pretu_x,pretu_z)\n",
    "            L2=torch.tensor(0,dtype=torch.float32).to(device)\n",
    "            if L2_lamda>0:\n",
    "                for name,parameters in model.named_parameters():\n",
    "                    if name[0:5]=='clase' and  name[-8:]=='0.weight':\n",
    "                        L2+=L2_lamda*torch.norm(parameters,2)\n",
    "            loss=0.5*(criterian1(yy,label)+criterian1(y,label))+L2\n",
    "            loss_sum+=loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (j+1)%10==0:\n",
    "            acc,f1,TP_test,TN_test,FP_test,FN_test=test(device,model,testloader)\n",
    "            result.loc[(j+1)//10]=[loss_sum,acc,f1,TP_test,TN_test,FP_test,FN_test]\n",
    "    result.sort_values('Acc',inplace=True,ascending=False)\n",
    "    print(' FinalAcc: {:.4f}'.format(result.iloc[0]['Acc']))\n",
    "    return result.iloc[0]['Acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 机构混合续\n",
    "train_site=test_site=np.load('DATAARRANGE/train_test_site.npy')\n",
    "train_asd_dict=np.load('DATAARRANGE/train_asd_dict.npy',allow_pickle=True).item()\n",
    "train_tdc_dict=np.load('DATAARRANGE/train_tdc_dict.npy',allow_pickle=True).item()\n",
    "test_asd_dict=np.load('DATAARRANGE/test_asd_dict.npy',allow_pickle=True).item()\n",
    "test_tdc_dict=np.load('DATAARRANGE/test_tdc_dict.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FinalAcc: 0.7765\n",
      " FinalAcc: 0.6863\n",
      " FinalAcc: 0.6634\n",
      " FinalAcc: 0.6893\n",
      " FinalAcc: 0.7670\n",
      " FinalAcc: 0.6875\n",
      " FinalAcc: 0.7200\n",
      " FinalAcc: 0.6346\n",
      " FinalAcc: 0.6869\n",
      " FinalAcc: 0.6842\n",
      " FinalAcc: 0.7529\n",
      " FinalAcc: 0.7157\n",
      " FinalAcc: 0.7030\n",
      " FinalAcc: 0.7087\n",
      " FinalAcc: 0.7282\n",
      " FinalAcc: 0.7083\n",
      " FinalAcc: 0.6900\n",
      " FinalAcc: 0.6538\n",
      " FinalAcc: 0.6768\n",
      " FinalAcc: 0.6930\n",
      " FinalAcc: 0.7412\n",
      " FinalAcc: 0.7255\n",
      " FinalAcc: 0.7426\n",
      " FinalAcc: 0.6990\n",
      " FinalAcc: 0.6990\n",
      " FinalAcc: 0.7083\n",
      " FinalAcc: 0.7000\n",
      " FinalAcc: 0.6731\n",
      " FinalAcc: 0.6970\n",
      " FinalAcc: 0.6579\n",
      " FinalAcc: 0.7529\n",
      " FinalAcc: 0.6863\n",
      " FinalAcc: 0.6931\n",
      " FinalAcc: 0.6893\n",
      " FinalAcc: 0.7087\n",
      " FinalAcc: 0.6979\n",
      " FinalAcc: 0.6900\n",
      " FinalAcc: 0.6250\n",
      " FinalAcc: 0.6768\n",
      " FinalAcc: 0.7018\n",
      " FinalAcc: 0.7647\n",
      " FinalAcc: 0.7353\n",
      " FinalAcc: 0.7228\n",
      " FinalAcc: 0.6990\n",
      " FinalAcc: 0.7184\n",
      " FinalAcc: 0.7083\n",
      " FinalAcc: 0.7100\n",
      " FinalAcc: 0.6635\n",
      " FinalAcc: 0.7374\n",
      " FinalAcc: 0.7193\n",
      " FinalAcc: 0.7765\n",
      " FinalAcc: 0.7647\n",
      " FinalAcc: 0.6832\n",
      " FinalAcc: 0.7184\n",
      " FinalAcc: 0.6990\n",
      " FinalAcc: 0.6979\n",
      " FinalAcc: 0.7000\n",
      " FinalAcc: 0.6538\n",
      " FinalAcc: 0.7172\n",
      " FinalAcc: 0.6579\n",
      " FinalAcc: 0.7647\n",
      " FinalAcc: 0.7157\n",
      " FinalAcc: 0.7327\n",
      " FinalAcc: 0.7282\n",
      " FinalAcc: 0.7670\n",
      " FinalAcc: 0.6667\n",
      " FinalAcc: 0.7000\n",
      " FinalAcc: 0.6250\n",
      " FinalAcc: 0.6970\n",
      " FinalAcc: 0.6754\n",
      " FinalAcc: 0.7765\n",
      " FinalAcc: 0.7059\n",
      " FinalAcc: 0.7129\n",
      " FinalAcc: 0.7087\n",
      " FinalAcc: 0.6796\n",
      " FinalAcc: 0.7083\n",
      " FinalAcc: 0.7100\n",
      " FinalAcc: 0.6538\n",
      " FinalAcc: 0.6970\n",
      " FinalAcc: 0.6579\n",
      " FinalAcc: 0.7882\n",
      " FinalAcc: 0.7255\n",
      " FinalAcc: 0.6832\n",
      " FinalAcc: 0.6796\n",
      " FinalAcc: 0.6990\n",
      " FinalAcc: 0.6979\n",
      " FinalAcc: 0.7300\n",
      " FinalAcc: 0.7019\n",
      " FinalAcc: 0.6869\n",
      " FinalAcc: 0.7018\n",
      " FinalAcc: 0.7765\n",
      " FinalAcc: 0.7059\n",
      " FinalAcc: 0.7228\n",
      " FinalAcc: 0.7379\n",
      " FinalAcc: 0.7184\n",
      " FinalAcc: 0.6875\n",
      " FinalAcc: 0.7300\n",
      " FinalAcc: 0.6442\n",
      " FinalAcc: 0.7172\n",
      " FinalAcc: 0.6667\n",
      " FinalAcc: 0.7529\n",
      " FinalAcc: 0.7255\n",
      " FinalAcc: 0.7327\n",
      " FinalAcc: 0.6893\n",
      " FinalAcc: 0.7184\n",
      " FinalAcc: 0.7188\n",
      " FinalAcc: 0.7100\n",
      " FinalAcc: 0.6827\n",
      " FinalAcc: 0.6667\n",
      " FinalAcc: 0.7018\n",
      " FinalAcc: 0.8118\n",
      " FinalAcc: 0.7353\n",
      " FinalAcc: 0.7327\n",
      " FinalAcc: 0.6699\n",
      " FinalAcc: 0.7282\n",
      " FinalAcc: 0.7708\n",
      " FinalAcc: 0.6600\n",
      " FinalAcc: 0.6346\n",
      " FinalAcc: 0.7071\n",
      " FinalAcc: 0.7018\n",
      " FinalAcc: 0.8000\n",
      " FinalAcc: 0.7353\n",
      " FinalAcc: 0.7228\n",
      " FinalAcc: 0.6990\n",
      " FinalAcc: 0.7379\n",
      " FinalAcc: 0.6875\n",
      " FinalAcc: 0.7600\n",
      " FinalAcc: 0.6442\n",
      " FinalAcc: 0.6667\n",
      " FinalAcc: 0.6754\n"
     ]
    }
   ],
   "source": [
    "setup_seed(123)\n",
    "global max_acc\n",
    "global saveModel\n",
    "global modelname\n",
    "for i in range(13):\n",
    "    L1_lamda=0.0\n",
    "    L2_lamda=0.0001\n",
    "    learning_rate=0.0001\n",
    "    epoch   =100\n",
    "    batch_size=64\n",
    "    layer   =5\n",
    "    for index in range(10):\n",
    "        saveModel=True\n",
    "        max_acc=0.6\n",
    "        modelname='../SAVEDModels/PGDtrainedensamble/models_{}_{}'.format(i,index)\n",
    "        train_asd=train_asd_dict[index]\n",
    "        train_tdc=train_tdc_dict[index]\n",
    "        test_asd =test_asd_dict[index]\n",
    "        test_tdc =test_tdc_dict[index]\n",
    "        trainset=dataset(site=train_site,fmri_root=cpac_root,smri_root=smri_root,ASD=train_asd,TDC=train_tdc)\n",
    "        trainloader=DataLoader(trainset,batch_size=batch_size,shuffle=True)\n",
    "        testset=dataset(site=test_site,fmri_root=cpac_root,smri_root=smri_root,ASD=test_asd,TDC=test_tdc)\n",
    "        testloader=DataLoader(testset,batch_size=1)\n",
    "        model=ANEGCN(layer).to(device)\n",
    "        train_pgd(model,trainloader,testloader,eps=0.02,iters=10,alpha=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7358490566037735\n",
      "0.6798336798336798\n",
      "0.7870722433460076\n"
     ]
    }
   ],
   "source": [
    "Pred_label={}\n",
    "True_label={}\n",
    "vote=13\n",
    "for i in range(vote):\n",
    "    Pred_label={}\n",
    "    True_label={}\n",
    "    for index in range(10):\n",
    "        PATH='../SAVEDModels/PGDtrainedensamble/models_{}_{}'.format(i,index)\n",
    "        test_asd =test_asd_dict[index]\n",
    "        test_tdc =test_tdc_dict[index]\n",
    "        testset=dataset(site=test_site,fmri_root=cpac_root,smri_root=smri_root,ASD=test_asd,TDC=test_tdc)\n",
    "        testloader=DataLoader(testset,batch_size=1)\n",
    "        model=ANEGCN(5)\n",
    "        model.load_state_dict(torch.load(PATH))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (X,Z,label,sub_id) in testloader:\n",
    "                True_label[sub_id.item()]=label.item()\n",
    "                y=model(X,Z)\n",
    "                _,predict=torch.max(y,1)\n",
    "                if sub_id.item() not in Pred_label:\n",
    "                    Pred_label[sub_id.item()]=0\n",
    "                if predict.item()==1:\n",
    "                    Pred_label[sub_id.item()]+=1      \n",
    "TP,TN,FP,FN=0,0,0,0\n",
    "for sId in True_label:\n",
    "    if True_label[sId]==1:\n",
    "        if Pred_label[sId]>= ((vote+1)//2):\n",
    "            TP+=1\n",
    "        else:\n",
    "            FN+=1\n",
    "    else:\n",
    "        if Pred_label[sId]<= ((vote-1)//2):\n",
    "            TN+=1\n",
    "        else:\n",
    "            FP+=1\n",
    "print((TP+TN)/1007)\n",
    "print(TP/(TP+FN))\n",
    "print(TN/(FP+TN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03: 0.7229;\n",
    "07: 0.7319;0.6736;0.7852;\n",
    "13: 0.7329;0.6881;0.7738;\n",
    "15: 0.7309;0.6861;0.7717;\n",
    "17: 0.7269;0.6798;0.7700;\n",
    "19: 0.7239;0.6775;0.7662;\n",
    "21: 0.7269;0.6819;0.7681;\n",
    "23: 0.7239;0.6840;0.7605;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13: 0.7468 0.7173 0.7738;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.6996+0.0415  Sen: 0.6698+0.0771  Spe: 0.7268+0.0702\n",
      "Acc: 0.7030+0.0259  Sen: 0.6305+0.0560  Spe: 0.7688+0.0477\n",
      "Acc: 0.7044+0.0256  Sen: 0.6602+0.0970  Spe: 0.7442+0.0728\n",
      "Acc: 0.6922+0.0298  Sen: 0.6183+0.1002  Spe: 0.7579+0.0887\n",
      "Acc: 0.7179+0.0252  Sen: 0.6572+0.1010  Spe: 0.7750+0.0571\n",
      "Acc: 0.7069+0.0379  Sen: 0.6794+0.0766  Spe: 0.7311+0.0842\n",
      "Acc: 0.7072+0.0420  Sen: 0.6651+0.1061  Spe: 0.7464+0.0674\n",
      "Acc: 0.7011+0.0326  Sen: 0.6562+0.0630  Spe: 0.7409+0.0696\n",
      "Acc: 0.7094+0.0306  Sen: 0.6657+0.1211  Spe: 0.7506+0.0757\n",
      "Acc: 0.7107+0.0355  Sen: 0.6757+0.0976  Spe: 0.7414+0.1029\n",
      "Acc: 0.7099+0.0242  Sen: 0.6797+0.0601  Spe: 0.7366+0.0465\n",
      "Acc: 0.7152+0.0501  Sen: 0.6783+0.0989  Spe: 0.7472+0.0879\n",
      "Acc: 0.7445+0.0929  Sen: 0.6958+0.1601  Spe: 0.7891+0.0919\n"
     ]
    }
   ],
   "source": [
    "vote=13\n",
    "for i in range(vote):\n",
    "    Acc,Sen,Spe=np.zeros(10),np.zeros(10),np.zeros(10)\n",
    "    for index in range(10):\n",
    "        PATH='../SAVEDModels/PGDtrainedensamble/models_{}_{}'.format(i,index)\n",
    "        test_asd =test_asd_dict[index]\n",
    "        test_tdc =test_tdc_dict[index]\n",
    "        testset=dataset(site=test_site,fmri_root=cpac_root,smri_root=smri_root,ASD=test_asd,TDC=test_tdc)\n",
    "        testloader=DataLoader(testset,batch_size=1)\n",
    "        model=ANEGCN(5)\n",
    "        model.load_state_dict(torch.load(PATH))\n",
    "        model.eval()\n",
    "        model=model.to(device)\n",
    "        with torch.no_grad():\n",
    "            acc,f1,TP_test,TN_test,FP_test,FN_test=test(device,model,testloader)\n",
    "            Acc[index]=acc\n",
    "            Sen[index]=TP_test/(TP_test+FN_test)\n",
    "            Spe[index]=TN_test/(TN_test+FP_test)\n",
    "    print('Acc: %.4f+%.4f  Sen: %.4f+%.4f  Spe: %.4f+%.4f'%(Acc.mean(),Acc.std(),Sen.mean(),Sen.std(),Spe.mean(),Spe.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011470317840576172 0.06524252891540527\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import numpy as np\n",
    "A=np.random.randn(100000)\n",
    "start=time.time()\n",
    "B=2*A\n",
    "end=time.time()\n",
    "time1=end-start\n",
    "start=time.time()\n",
    "B=[2*i for i in A]\n",
    "end=time.time()\n",
    "time2=end-start\n",
    "print(time1,time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
