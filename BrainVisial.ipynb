{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "device=torch.device('cpu')\n",
    "cpac_root='/media/dm/0001A094000BF891/Yazid/ABIDEI_CPAC/'\n",
    "smri_root='/media/dm/0001A094000BF891/Yazid/ABIDEI_sMRI/'\n",
    "nan_subid=np.load('nan_subid.npy').tolist()\n",
    "aal=np.load('Atlas/AAL.npy').tolist()\n",
    "Lobe_aal=np.load('SA-result/Lobe_aal.npy',allow_pickle=True).tolist()\n",
    "Lobe={}\n",
    "Lobe['Central']=[1,2,57,58,17,18]\n",
    "Lobe['Frontal']=[3,4,5,6,7,8,9,10,11,12,13,14,15,16,19,20,21,22,23,24,25,26,27,28,69,70]\n",
    "Lobe['Temporal']=[79,80,81,82,85,86,89,90]\n",
    "Lobe['Parietal']=[59,60,61,62,63,64,65,66,67,68]\n",
    "Lobe['Occipital']=[43,44,45,46,47,48,49,50,51,52,53,54,55,56]\n",
    "Lobe['Limbic']=[31,32,33,34,35,36,37,38,39,40,83,84,87,88]\n",
    "Lobe['Insula']=[29,30]\n",
    "Lobe['Subcortical']=[41,42,71,72,73,74,75,76,77,78]\n",
    "Lobe['Cerebelum']=[91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108]\n",
    "Lobe['Vermis']=[109,110,111,112,113,114,115,116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention,self).__init__()\n",
    "        self.conv1=nn.Conv1d(in_channels=3,out_channels=3,kernel_size=1,padding=0)\n",
    "        self.conv2=nn.Conv1d(in_channels=116,out_channels=116,kernel_size=1,padding=0)\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "    def forward(self,Z,X):\n",
    "        batchsize,x_dim,x_c= X.size()\n",
    "        batchsize,z_dim,z_c= Z.size()\n",
    "        K=self.conv1(X.permute(0,2,1))# BS,x_c,x_dim\n",
    "        Q=K.permute(0,2,1)# BS,x_dim,x_c\n",
    "        V=self.conv2(Z.permute(0,2,1))# Bs,z_c,z_dim\n",
    "        attention=self.softmax(torch.matmul(Q,K))#BS,x_dim,x_dim\n",
    "        out=torch.bmm(attention,V).permute(0,2,1)#BS,z_dim,z_c\n",
    "        return out\n",
    "class NEResGCN(nn.Module):\n",
    "    def __init__(self,layer):\n",
    "        super(NEResGCN,self).__init__()\n",
    "        self.layer =layer\n",
    "        self.relu  =nn.ReLU()\n",
    "        self.atten =nn.ModuleList([Attention() for i in range(layer)])\n",
    "        self.norm_n=nn.ModuleList([nn.BatchNorm1d(116) for i in range(layer)])\n",
    "        self.norm_e=nn.ModuleList([nn.BatchNorm1d(116) for i in range(layer)])\n",
    "        self.node_w=nn.ParameterList([nn.Parameter(torch.randn((3,3),dtype=torch.float32)) for i in range(layer)])\n",
    "        self.edge_w=nn.ParameterList([nn.Parameter(torch.randn((116,116),dtype=torch.float32)) for i in range(layer)])\n",
    "        self.line_n=nn.ModuleList([nn.Sequential(nn.Linear(116*3,128),nn.ReLU(),nn.BatchNorm1d(128)) for i in range(layer+1)])\n",
    "        self.line_e=nn.ModuleList([nn.Sequential(nn.Linear(116*116,128*3),nn.ReLU(),nn.BatchNorm1d(128*3)) for i in range(layer+1)])\n",
    "        self.clase =nn.Sequential(nn.Linear(128*4*(self.layer+1),1024),nn.Dropout(0.2),nn.ReLU(),\n",
    "                                   nn.Linear(1024,2))\n",
    "        self.ones=nn.Parameter(torch.ones((116),dtype=torch.float32),requires_grad=False)\n",
    "        self._initialize_weights()\n",
    "    # params initialization\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv1d,nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def normalized(self,Z):\n",
    "        n=Z.size()[0]\n",
    "        A=Z[0,:,:]\n",
    "        A=A+torch.diag(self.ones)\n",
    "        d=A.sum(1)\n",
    "        D=torch.diag(torch.pow(d,-1))\n",
    "        A=D.mm(A).reshape(1,116,116)\n",
    "        for i in range(1,n):\n",
    "            A1=Z[i,:,:]+torch.diag(self.ones)\n",
    "            d=A1.sum(1)\n",
    "            D=torch.diag(torch.pow(d,-1))\n",
    "            A1=D.mm(A1).reshape(1,116,116)\n",
    "            A=torch.cat((A,A1),0)\n",
    "        return A\n",
    "        \n",
    "    def update_A(self,Z):\n",
    "        n=Z.size()[0]\n",
    "        A=Z[0,:,:]\n",
    "        Value,_=torch.topk(torch.abs(A.view(-1)),int(116*116*0.2))\n",
    "        A=(torch.abs(A)>=Value[-1])+torch.tensor(0,dtype=torch.float32)\n",
    "        A=A.reshape(1,116,116)\n",
    "        for i in range(1,n):\n",
    "            A2=Z[i,:,:]\n",
    "            Value,_=torch.topk(torch.abs(A2.view(-1)),int(116*116*0.2))\n",
    "            A2=(torch.abs(A2)>=Value[-1])+torch.tensor(0,dtype=torch.float32)\n",
    "            A2=A2.reshape(1,116,116)\n",
    "            A=torch.cat((A,A2),0)\n",
    "        return A\n",
    "        \n",
    "    def forward(self,X,Z):\n",
    "        n=X.size()[0]\n",
    "        XX=self.line_n[0](X.view(n,-1))\n",
    "        ZZ=self.line_e[0](Z.view(n,-1))\n",
    "        for i in range(self.layer):\n",
    "            A=self.atten[i](Z,X)\n",
    "            Z1=torch.matmul(A,Z)\n",
    "            Z2=torch.matmul(Z1,self.edge_w[i])\n",
    "            Z=self.relu(self.norm_e[i](Z2))+Z\n",
    "            #Z.register_hook(grad_Z_hook)\n",
    "            #feat_Z_hook(Z)\n",
    "            ZZ=torch.cat((ZZ,self.line_e[i+1](Z.view(n,-1))),dim=1)\n",
    "            X1=torch.matmul(A,X)\n",
    "            X1=torch.matmul(X1,self.node_w[i])\n",
    "            X=self.relu(self.norm_n[i](X1))+X\n",
    "            #X.register_hook(grad_X_hook)\n",
    "            #feat_X_hook(X)\n",
    "            XX=torch.cat((XX,self.line_n[i+1](X.view(n,-1))),dim=1)\n",
    "        XZ=torch.cat((XX,ZZ),1)\n",
    "        y=self.clase(XZ)\n",
    "        #print(self.clase[0].weight)\n",
    "        return y\n",
    "# def grad_X_hook(grad):\n",
    "#     X_grad.append(grad)\n",
    "# def feat_X_hook(X):\n",
    "#     X_feat.append(X.detach())\n",
    "# def grad_Z_hook(grad):\n",
    "#     Z_grad.append(grad)\n",
    "# def feat_Z_hook(Z):\n",
    "#     Z_feat.append(Z.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalized(X):\n",
    "    return (X-X.mean())/X.std()\n",
    "def gradient(device,model,dataloader):\n",
    "    model.eval()\n",
    "    result_asd_x=np.zeros((116,3))\n",
    "    result_asd_z=np.zeros((116,116))\n",
    "    result_tdc_x=np.zeros((116,3))\n",
    "    result_tdc_z=np.zeros((116,116))\n",
    "    for (X,Z,A,label,sub_id) in dataloader:\n",
    "        model.zero_grad()\n",
    "        X=torch.autograd.Variable(X,requires_grad=True)\n",
    "        x=X.to(device)\n",
    "        Z=torch.autograd.Variable(Z,requires_grad=True)\n",
    "        z=Z.to(device)\n",
    "        A=torch.autograd.Variable(A,requires_grad=True)\n",
    "        a=A.to(device)\n",
    "        y=model(x,z)\n",
    "        if (label==torch.FloatTensor([0])).item():\n",
    "            torch.autograd.backward(y,torch.FloatTensor([[1.,0.]]).to(device))\n",
    "        else:\n",
    "            torch.autograd.backward(y,torch.FloatTensor([[0.,1.]]).to(device))\n",
    "        grad_X=X.grad.numpy()[0]\n",
    "        grad_Z=Z.grad.numpy()[0]\n",
    "        grad_X=normalized(grad_X)\n",
    "        grad_Z=normalized(grad_Z)\n",
    "        if label==torch.FloatTensor([0]).item():\n",
    "            result_tdc_x+=grad_X\n",
    "            result_tdc_z+=grad_Z\n",
    "        else:\n",
    "            result_asd_x+=grad_X\n",
    "            result_asd_z+=grad_Z\n",
    "    return result_asd_x,result_asd_z,result_tdc_x,result_tdc_z\n",
    "def grad_cam(grad,feat,top_rate):\n",
    "    N=len(grad)//5\n",
    "    n=grad[0].shape[2]\n",
    "    result=torch.zeros((116,n))\n",
    "    for i in range(N):\n",
    "        weight=torch.zeros(5)\n",
    "        for j in range(5):\n",
    "            weight[j]=(grad[i*5+j]*(grad[i*5+j]>0)).sum()\n",
    "        weight=F.softmax(weight, dim=0)\n",
    "        feature=torch.zeros((116,n))\n",
    "        for j in range(5):\n",
    "            feature+=weight[j]*(grad[i*5+j][0]>0)*grad[i*5+j][0]\n",
    "        value_x,_=torch.topk(torch.abs(feature.view(-1)),int(116*n*top_rate))\n",
    "        result+=(torch.abs(feature)>=value_x[-1])\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_arange(sites,fmri_root,smri_root,nan_subid):\n",
    "    asd,tdc=[],[]\n",
    "    for site in sites:\n",
    "        mri_asd=os.listdir(smri_root+site+'/group1')\n",
    "        mri_tdc=os.listdir(smri_root+site+'/group2')\n",
    "        fmri_asd=os.listdir(fmri_root+site+'/group1_FC')\n",
    "        fmri_tdc=os.listdir(fmri_root+site+'/group2_FC')\n",
    "        site_asd=[i for i in mri_asd if i in fmri_asd ]\n",
    "        site_tdc=[i for i in mri_tdc if i in fmri_tdc ]\n",
    "        site_asd=[i for i in site_asd if int(i[:5]) not in nan_subid]\n",
    "        site_tdc=[i for i in site_tdc if int(i[:5]) not in nan_subid]\n",
    "        asd.append(site_asd)\n",
    "        tdc.append(site_tdc) \n",
    "    return asd,tdc\n",
    "class dataset(Dataset):\n",
    "    def __init__(self,fmri_root,smri_root,site,ASD,TDC,topk=True,rate=0.2):\n",
    "        super(dataset,self).__init__()\n",
    "        self.fmri=fmri_root\n",
    "        self.smri=smri_root\n",
    "        self.ASD=[j for i in ASD for j in i]\n",
    "        self.TDC=[j for i in TDC for j in i]\n",
    "        self.data=self.ASD+self.TDC\n",
    "        random.shuffle(self.data)\n",
    "        self.data_site={}\n",
    "        for i in range(len(site)):\n",
    "            data=ASD[i]+TDC[i]\n",
    "            for j in data:\n",
    "                if j not in self.data_site:\n",
    "                    self.data_site[j]=site[i]                \n",
    "        self.rate=rate\n",
    "        self.topk=topk\n",
    "    def normalize(self,A):\n",
    "        d=A.sum(1)\n",
    "        D=torch.diag(torch.pow(d,-1))\n",
    "        return D.mm(A)\n",
    "    def __getitem__(self,index):\n",
    "        data=self.data[index]\n",
    "        sub_id=int(data[0:5])\n",
    "        if data in self.ASD:\n",
    "            data_slow5 =np.load(self.fmri+self.data_site[data]+'/group1_slow5/'+data,allow_pickle=True)\n",
    "            data_slow4 =np.load(self.fmri+self.data_site[data]+'/group1_slow4/'+data,allow_pickle=True)\n",
    "            data_voxel =np.load(self.smri+self.data_site[data]+'/group1/'+data,allow_pickle=True)\n",
    "            data_FCz   =np.load(self.fmri+self.data_site[data]+'/group1_FC/'+data,allow_pickle=True)\n",
    "        elif data in self.TDC:\n",
    "            data_slow5 =np.load(self.fmri+self.data_site[data]+'/group2_slow5/'+data,allow_pickle=True)\n",
    "            data_slow4 =np.load(self.fmri+self.data_site[data]+'/group2_slow4/'+data,allow_pickle=True)\n",
    "            data_voxel =np.load(self.smri+self.data_site[data]+'/group2/'+data,allow_pickle=True)\n",
    "            data_FCz   =np.load(self.fmri+self.data_site[data]+'/group2_FC/'+data,allow_pickle=True)\n",
    "        else:\n",
    "            print('wrong input')\n",
    "        data_slow5=(data_slow5-np.min(data_slow5))/(np.max(data_slow5)-np.min(data_slow5))\n",
    "        data_slow4=(data_slow4-np.min(data_slow4))/(np.max(data_slow4)-np.min(data_slow4))\n",
    "        if np.any(np.isnan(data_slow5)) or np.any(np.isnan(data_slow4)) or np.any(np.isnan(data_FCz)):\n",
    "            print('data wronmg')\n",
    "        #data_FCz=(data_FCz-np.min(data_FCz))/(np.max(data_FCz)-np.min(data_FCz))\n",
    "        if self.data[index] in self.ASD:\n",
    "            label=torch.tensor(1)\n",
    "        else:\n",
    "            label=torch.tensor(0)\n",
    "        X=np.zeros((116,3),dtype=np.float32)\n",
    "        X[:,0]=data_slow5\n",
    "        X[:,1]=data_slow4\n",
    "        X[:,2]=data_voxel\n",
    "        data_FCz=data_FCz.astype(np.float32)\n",
    "        Z=torch.from_numpy(data_FCz)\n",
    "        X=torch.from_numpy(X)\n",
    "        if self.topk:\n",
    "            Value,_=torch.topk(torch.abs(Z.view(-1)),int(116*116*self.rate))\n",
    "            A=(torch.abs(Z)>=Value[-1])+torch.tensor(0.0,dtype=torch.float32)\n",
    "        A=self.normalize(A)\n",
    "        return X,Z,A,label,sub_id\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gradient-basedfor\n",
    "featuresMap=list()\n",
    "all_site=os.listdir(cpac_root)\n",
    "Result_ASD_X,Result_TDC_X=np.zeros((116,3)),np.zeros((116,3))\n",
    "Result_TDC_Z,Result_ASD_Z=np.zeros((116,116)),np.zeros((116,116))\n",
    "vote = 13\n",
    "for i in range(vote):\n",
    "    for index in range(10):\n",
    "        PATH='/media/dm/0001A094000BF891/Yazid/SAVEDModels/ensamble/models_{}_{}'.format(i,index)\n",
    "        model=NEResGCN(5)\n",
    "        model.load_state_dict(torch.load(PATH))\n",
    "        train_asd,train_tdc=data_arange(all_site,fmri_root=cpac_root,smri_root=smri_root,nan_subid=nan_subid)\n",
    "        trainset=dataset(site=all_site,fmri_root=cpac_root,smri_root=smri_root,ASD=train_asd,TDC=train_tdc)\n",
    "        trainloader=DataLoader(trainset,batch_size=1,shuffle=True,drop_last=True)\n",
    "        result_asd_x,result_asd_z,result_tdc_x,result_tdc_z=gradient(device,model,trainloader)\n",
    "        Result_ASD_X+=result_asd_x\n",
    "        Result_ASD_Z+=result_asd_z\n",
    "        Result_TDC_X+=result_tdc_x\n",
    "        Result_TDC_Z+=result_tdc_z\n",
    "np.save('SA-result/ensamble/adTrained_asd_x_{}.npy'.format(vote),Result_ASD_X)\n",
    "np.save('SA-result/ensamble/adTrained_asd_z_{}.npy'.format(vote),Result_ASD_Z)\n",
    "np.save('SA-result/ensamble/adTrained_tdc_x_{}.npy'.format(vote),Result_TDC_X)\n",
    "np.save('SA-result/ensamble/adTrained_tdc_z_{}.npy'.format(vote),Result_TDC_Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
